# TEXT-SUMMARIZATION-TOOL

*NAME: SRIKAR BHARADWAJ

*DESCRIPTION:  

*Python 3 – Core programming language

*Transformers (Hugging Face) – For T5 model and tokenizer

*PyTorch – Deep learning framework to run the model

*T5-base – Pretrained model for text summarization

*CLI / Terminal – To run the script    
(When working with cutting-edge machine learning, particularly in the field of Natural Language Processing (NLP), two libraries frequently come into play: transformers and torch. The command pip install transformers torch installs these powerful Python libraries and sets the foundation for developing AI models like GPT, BERT, T5, and others.

What is transformers?
The transformers library is developed and maintained by Hugging Face, a leading company in open-source AI research. This library provides easy-to-use APIs to access thousands of pre-trained models for tasks like:

Text classification (e.g., spam detection, sentiment analysis)

Summarization

Question answering

Translation

Text generation (e.g., chatbots)

Token classification (e.g., named entity recognition)

Transformers are a type of deep learning model introduced in the 2017 paper “Attention is All You Need”. They have since become the gold standard for handling sequential data like text. Hugging Face’s transformers library wraps many of these models and makes them accessible with just a few lines of code. You can load models like GPT-2, BERT, T5, and DistilBERT with one command, perform inference, and even fine-tune them on your own datasets.

For example:
from transformers import pipeline
summarizer = pipeline("summarization")
result = summarizer("Your long article here.")
This level of abstraction makes state-of-the-art NLP accessible to researchers, developers, and businesses.

What is torch?
torch, short for PyTorch, is an open-source machine learning framework developed by Meta (Facebook AI). It is widely used in both academic research and industry. PyTorch offers a flexible, dynamic computation graph that makes it easier to debug and iterate on models compared to static graph frameworks like TensorFlow (though TensorFlow has evolved significantly since).

PyTorch is used to:

Build deep learning models from scratch

Train and fine-tune neural networks

Perform tensor operations (like NumPy, but with GPU support)

Leverage GPU acceleration for faster computation

Work with high-level APIs like torch.nn for building complex architectures

In essence, while transformers is like a toolbox filled with pre-trained AI models, torch is the engine that makes those models run.

Installing the Libraries
pip install transformers torch
downloads and installs both packages along with their dependencies. This step is crucial before using any Hugging Face model, as the transformers library relies on PyTorch (or TensorFlow) as the underlying framework.

After installation, you’ll be able to:

Load and run pre-trained models

Tokenize and preprocess text

Train custom models with your own data

Export or deploy models into production environments

Installing transformers and torch equips you with tools to explore state-of-the-art AI, build intelligent applications, or research the next breakthrough in NLP. Together, they form the core of modern deep learning for language understanding and generation. Whether you're a beginner in AI or a seasoned developer, this installation is your gateway to powerful and scalable machine learning solutions.

* OUTPUT:  ![Image](https://github.com/user-attachments/assets/9728d0dd-07a7-4416-abfb-0963c6d65012)
* ![Image](https://github.com/user-at

![Image](https://github.com/user-attachments/assets/da8a55cc-9f63-4bab-9fd7-e1f6fe802489)

tachments/assets/9728d0dd-07a7-4416-abfb-0963c6d65012)
*  ![image](https://github.com/user-attachments/assets/0f0aebef-0b0b-4008-a06e-99f791dd7209)

